# V1 Roadmap: E2E Testable Basic Application

## V1 Goals

The primary goal for V1 is to deliver an end-to-end (E2E) testable version of the application that demonstrates core agentic workflow and **proves the viability of LLM-based code generation for a defined, non-trivial task.** This version will focus on implementing the core workflow and ensuring that all fundamental components, including LLM integration for coding, are integrated and functioning correctly.

Key objectives for V1 include:
-   Establish a stable and repeatable E2E testing process.
-   Implement the core sequential execution loop of the orchestrator.
-   **Demonstrate viable LLM-based code generation for a defined, non-trivial task beyond simple "Hello, World!" examples.**
-   Develop functional V1 versions of essential agent modules (Planner, Coder, Auditor), **with the Coder agent utilizing LLMs for code generation.**
-   Ensure seamless communication and data flow between agents and the orchestrator.
-   Provide clear documentation for setting up and running the V1 application.

## Key V1 Components/Modules and Branches

The following components and modules are critical for V1. Each will be developed in a dedicated feature branch:

-   [x] `feature/V1-orchestrator-loop`: Implement the basic sequential execution loop for the orchestrator. This includes managing the state of the workflow and transitioning between different agent tasks.
-   [x] `feature/V1-planner-agent-basic`: Develop a rudimentary Planner agent capable of generating an initial plan based on a given prompt.
-   [x] `feature/V1-coder-agent-basic`: Create a Coder agent that utilizes LLM capabilities to take a plan and generate functional Python code for a defined, non-trivial task. This includes implementing logic, not just boilerplate or direct task echoing.
-   [x] `feature/V1-auditor-agent-basic`: Implement a simple Auditor agent that can perform static checks on the code generated by the Coder agent.
-   [x] `feature/V1-communication-protocol`: Define and implement the initial version of the communication protocol between the orchestrator and the agents.
-   [x] `feature/V1-e2e-testing-framework`: Set up the basic infrastructure and write initial E2E tests that cover the core application workflow.
-   [x] `feature/V1-initial-documentation`: Draft initial user guides for setting up the development environment and running the V1 application.

## Integration Points and Merge Strategy

**Integration Points:**
-   The Orchestrator will be the central hub, invoking Planner, Coder, and Auditor agents in sequence.
-   Planner output will be the input for the Coder.
-   Coder output (code) will be the input for the Auditor.
-   Auditor feedback may loop back to the Coder or be presented to the user.

**Merge Strategy:**
1.  Development will occur on individual `feature/V1-*` branches.
2.  Once a feature is complete and unit-tested, it will be merged into a `develop/V1` integration branch.
3.  Regular integration testing will be performed on `develop/V1`.
4.  Once all V1 features are integrated and E2E tests pass on `develop/V1`, this branch will be tagged as `v1.0.0-alpha` (or similar) and merged into the `main` branch.

## V1 E2E Test Specifications

The E2E tests for V1 will validate the complete workflow, including LLM-based code generation.

**Test Case 1: Foundational "Hello, World!" Application Generation (Sanity Check)**
1.  **Input:** A prompt requesting the generation of a simple "Hello, World" program in Python.
2.  **Planner Agent:**
    *   Generates a plan: e.g., "Create a Python file that prints 'Hello, World!'".
3.  **Coder Agent:**
    *   Receives the plan.
    *   Generates `main.py` with `print("Hello, World!")`. (This can still be a simpler path for the Coder or use LLM).
4.  **Auditor Agent:**
    *   Receives `main.py`.
    *   Performs basic static analysis (e.g., syntax check).
    *   Reports success if no major issues are found.
5.  **Orchestrator:**
    *   Successfully guides the process from prompt to audited code.
    *   Outputs the path to the generated and audited `main.py`.
6.  **Validation:**
    *   The generated `main.py` exists.
    *   Running `python main.py` prints "Hello, World!" to standard output.
    *   The audit report indicates success.

**Test Case 2: LLM-Based Code Generation for a Utility Application**
1.  **Input:** A prompt requesting a simple utility application, e.g., "Create a Python CLI tool that takes a text file as input and counts the occurrences of each word, then prints the top 5 words and their counts."
2.  **Planner Agent:**
    *   Generates a structured plan for the utility (e.g., steps: read file, process text, count words, find top N, print results).
3.  **Coder Agent:**
    *   Receives the plan.
    *   **Utilizes LLM capabilities** to generate Python code that implements the planned functionality.
    *   The generated code should be functional and handle basic scenarios (e.g., file not found).
4.  **Auditor Agent:**
    *   Receives the generated Python file(s).
    *   Performs static analysis (syntax check, linting).
    *   (Optional V1 enhancement) Basic functional check if feasible within V1 scope.
    *   Reports success if the code is syntactically correct and passes basic quality checks.
5.  **Orchestrator:**
    *   Successfully guides the process from prompt to audited code.
    *   Outputs the path to the generated and audited code.
6.  **Validation:**
    *   The generated Python script(s) exist.
    *   The script executes and produces the correct output for a sample input file.
    *   The audit report indicates success.

**Test Case 3: Handling a Simple Error/Correction Loop (Optional for initial V1, but desirable)**
1.  **Input:** A prompt that might lead to a correctable error by the Coder (could be for Test Case 1 or 2).
2.  **Scenario:** Similar to the above test cases, but the Coder (LLM or basic) initially produces code with a minor, detectable syntax error or simple logical flaw.
3.  **Auditor Agent:**
    *   Detects the error.
    *   Provides feedback to the Orchestrator/Coder.
4.  **Coder Agent (Iteration 2):**
    *   Receives feedback.
    *   Corrects the error (potentially with another LLM call if applicable).
5.  **Auditor Agent (Iteration 2):**
    *   Verifies the correction.
6.  **Validation:**
    *   The final generated code is correct and passes E2E validation as per the original test case.

These E2E tests will ensure that the core mechanics of the application, **including LLM-based code generation,** are sound and provide a baseline for future development.
