# AGENTS.md

**Project:** Gandalf Workshop

**Meta-Instructions for Future Jules Iterations:**
*   **At the end of your assigned task, you MUST perform these steps:**
    1.  **Overwrite this file (`AGENTS.md`)**: Do not create a new timestamped or uniquely named file. Always update this specific file.
    2.  **Summarize Your Accomplishments**: Briefly describe the task you just completed and the key changes you made or committed.
    3.  **Provide Clear Next Steps**: Based on the project's roadmap and your understanding of the current state, outline the most logical next task(s) for the subsequent Jules iteration.
    4.  **Include General Reminders**:
        *   Emphasize adherence to `CONTRIBUTING.md`.
        *   Point to the relevant roadmap document (e.g., `docs/roadmap/V1.md`, `docs/roadmap/V2.md`) for task identification.
        *   Remind to check alignment with the overall project vision (see `docs/VERSION_ROADMAP.md` and other strategic documents).
        *   Stress the importance of checking for and following instructions in any `AGENTS.md` files (like this one!).
*   **Your goal is to ensure a smooth handover to the next Jules, providing all necessary context and direction.**

---

**Your Current State & Context (as of this update):**

*   You (the current Jules iteration) have successfully refactored the LLM provider handling and integrated it into the main workflow, enabling the Coder agent to use live LLMs. This fulfills a critical part of the V1 goal: LLM-based code generation.
*   **Key Accomplishments:**
    1.  **`LLMProviderManager` Created (`gandalf_workshop/llm_provider_manager.py`):**
        *   Refactored the old `LLM_API_validator.py` into a robust `LLMProviderManager` class.
        *   This manager loads API keys from `.env` for Gemini, Together AI, and Mistral.
        *   It can check the operational status of these providers and list their available models.
        *   It provides a method (`get_llm_provider()`) to select a working LLM provider, with options for preferring a specific one.
    2.  **Integration with `WorkshopManager` (`gandalf_workshop/workshop_manager.py`):**
        *   `WorkshopManager` now instantiates `LLMProviderManager` upon its own initialization.
        *   It attempts to secure an LLM configuration. If no LLM provider is found, it logs a critical error and will prevent commissions from running.
        *   The obtained LLM configuration is now passed to `initialize_coder_agent_v1`.
    3.  **LLM Utilization in Coder Agent (`gandalf_workshop/artisan_guildhall/artisans.py`):**
        *   `initialize_coder_agent_v1` now accepts an `llm_config` parameter.
        *   If `llm_config` is provided, the Coder:
            *   Selects a suitable model from the provider's list (with improved logic to prefer modern/capable models like Gemini 1.5 Flash/Pro).
            *   Constructs a prompt using `CODER_CHARTER_PROMPT` and the task from the planner.
            *   Makes a live API call to the selected LLM provider (Gemini, Mistral, or Together AI) to generate Python code.
            *   Includes basic cleanup for markdown code blocks from the LLM response.
            *   Saves the generated code to `app.py` (or `app_llm_failed.py` if LLM call fails, ensuring a `.py` file for the auditor).
    4.  **Dependency Management:**
        *   Added `google-generativeai`, `together`, and `mistralai` to `requirements.in`.
        *   Ensured `pyyaml` was correctly handled (it was already in `requirements.in`; the issue was environment/installation).
        *   Used `make lock` and `make install` to update `requirements.txt` and install dependencies into the `.venv`.
    5.  **Testing:**
        *   Developed unit tests for `LLMProviderManager` (`gandalf_workshop/tests/test_llm_provider_manager.py`) that now test against *live* API services (after an initial attempt with mocks proved very complex to maintain for this iteration). Tests cover scenarios like key availability and provider selection. All unit tests are currently passing.
        *   Successfully ran a manual end-to-end test commission: `python main.py --prompt "Create a Python CLI tool that takes a text file as input and counts the occurrences of each word, then prints the top 5 words and their counts."` This resulted in functional Python code being generated by Gemini and saved to `outputs/commission_b79d5213/app.py`.
    6.  **Cleanup**: The old `LLM_API_validator.py` is ready to be deleted.

*   The system now dynamically selects an LLM provider at runtime and uses it for code generation, a major step towards V1 compliance.

**Next Steps & Focus Areas for the *Next* Jules Iteration:**

**1. Enhance Coder Agent's LLM Interaction & Reliability:**
    *   **Prompt Engineering**: The current prompt for the Coder is basic. Refine `CODER_CHARTER_PROMPT` and the way task descriptions are fed to the LLM to improve code quality, adherence to specifications (from `PlanOutput`), and consistency. Consider structured input/output for the LLM if possible.
    *   **Error Handling & Retries for LLM Calls**: Implement more robust error handling for LLM API calls within `initialize_coder_agent_v1` (e.g., retries for transient network issues, specific error parsing).
    *   **Model Capability Matching**: The `LLMProviderManager` has a placeholder for `required_capabilities`. Implement logic to actually use this, so it can select models based on needs (e.g., chat, function calling, specific context window size). The Coder should then be able to request a provider with certain capabilities.
    *   **Context Management**: For more complex tasks, the Coder LLM will need to manage larger contexts, potentially breaking down tasks further or using techniques to handle limited context windows.

**2. Planner Agent Enhancement (Supporting the LLM Coder):**
    *   As per the original `AGENTS.md` guidance: Ensure `initialize_planner_agent_v1` produces plans (`PlanOutput`) that are detailed and structured enough to be genuinely useful for the LLM-based Coder. The current placeholder planner is too simple for complex tasks. It should break down the user prompt into actionable steps for the Coder LLM. This might involve making the Planner itself use an LLM.

**3. Auditor Agent Enhancement:**
    *   Expand `initialize_auditor_agent_v1` beyond a simple syntax check. It should:
        *   Run linters (e.g., `flake8`, `pylint`) on the generated code.
        *   Potentially execute basic functional tests if the Coder can also generate them or if they can be inferred from the `PlanOutput`. (This aligns with `docs/roadmap/V1.md` Test Case 2 which implies testing the generated utility app).

**4. Iterative Development Loop (Post-V1 or Advanced V1):**
    *   Review `WorkshopManager` to better support an iterative loop where audit feedback (especially from a more advanced Auditor) can be passed back to the Coder for revisions using the LLM. This aligns with the "Generate â†” Critique" cycle in `high_level_design.md`.

**5. Configuration Management:**
    *   Consider moving LLM preferences (preferred provider, specific model preferences per task type) into a configuration file rather than hardcoding or relying solely on `LLMProviderManager`'s defaults.

**General Guidance:**
*   **Branching:** Use feature branches (e.g., `feature/V1.1-coder-prompt-refinement`, `feature/V1.1-planner-llm`) for new work.
*   **Testing:** Continue to write unit tests for new logic and ensure E2E tests (like the word count CLI tool) remain functional or are expanded.
*   **Consult Documentation:** Refer to `docs/roadmap/V1.md`, `docs/1_strategic_overview/high_level_design.md`, and artisan charters.
*   **AGENTS.md:** Update this file with your progress and next steps.

This iteration has laid a strong foundation for LLM-driven development in the workshop!
[end of AGENTS.md]
