# AGENTS.md

**Project:** Gandalf Workshop

**Meta-Instructions for Future Jules Iterations:**
*   **At the end of your assigned task, you MUST perform these steps:**
    1.  **Overwrite this file (`AGENTS.md`)**: Do not create a new timestamped or uniquely named file. Always update this specific file.
    2.  **Summarize Your Accomplishments**: Briefly describe the task you just completed and the key changes you made or committed.
    3.  **Provide Clear Next Steps**: Based on the project's roadmap and your understanding of the current state, outline the most logical next task(s) for the subsequent Jules iteration.
    4.  **Include General Reminders**:
        *   Emphasize adherence to `CONTRIBUTING.md`.
        *   Point to the relevant roadmap document (e.g., `docs/roadmap/V1.md`, `docs/roadmap/V2.md`) for task identification.
        *   Remind to check alignment with the overall project vision (see `docs/VERSION_ROADMAP.md` and other strategic documents).
        *   Stress the importance of checking for and following instructions in any `AGENTS.md` files (like this one!).
*   **Your goal is to ensure a smooth handover to the next Jules, providing all necessary context and direction.**

---

## Task Summary: Implemented Live LLM Agent Chain with Gemini

Jules successfully upgraded the Gandalf Workshop's V1 pipeline to use live LLM agents (Google Gemini) for planning and coding, replacing the previous mock implementations.

**Key Changes Made:**

1.  **API Key Management:**
    *   Added `_get_gemini_api_key()` in `gandalf_workshop/artisan_guildhall/artisans.py` to securely access the `GEMINI_API_KEY` from environment variables.
    *   **Important:** The successful operation relies on the `GEMINI_API_KEY` being set in a `.env` file at the project root or as an environment variable. During development, there were issues with `.env` file persistence in the environment, which required temporary hardcoding for testing. This hardcoding has been removed.

2.  **Gemini Integration:**
    *   Added `google-generativeai` to `requirements.txt` and installed it.
    *   Modified `initialize_live_planner_agent` in `artisans.py`:
        *   Uses `google.generativeai` to call a Gemini model (e.g., `gemini-1.5-flash-latest`).
        *   Passes `PLANNER_CHARTER_PROMPT` and the user prompt to Gemini.
        *   Parses the plan from the Gemini response.
    *   Modified `initialize_live_coder_agent` in `artisans.py`:
        *   Uses `google.generativeai` to call a Gemini model.
        *   Passes `CODER_CHARTER_PROMPT` and the plan to Gemini.
        *   Saves the generated code from Gemini's response to `generated_code.py`.
    *   Updated `workshop_manager.py` to call these new live agent functions.

3.  **Auditor Agent:**
    *   Confirmed that the existing `initialize_auditor_agent_v1` (syntax check) is correctly called after the live coder agent.

4.  **Testing:**
    *   Successfully tested the new live agent pipeline with both simple ("add two numbers") and complex ("streamlit calculator app") prompts.
    *   The generated code was functional and passed the syntax audit.

5.  **Documentation:**
    *   Updated docstrings in `artisans.py` for the new live agent functions to reflect the use of Gemini.
    *   This `AGENTS.md` file has been updated.

---

## Next Steps for Gandalf Workshop

Based on the current state and project roadmap, the following tasks are recommended for the next Jules iteration:

1.  **Robust `.env` File Handling & API Key Management:**
    *   Investigate and ensure reliable loading of API keys from `.env` files across all execution environments. The previous iteration faced challenges with this.
    *   Consider implementing a more centralized configuration management for API keys and other settings if the project complexity grows.

2.  **Implement Live Auditor Agent (LLM-based Semantic Audit):**
    *   The current auditor only performs a syntax check (`py_compile`).
    *   Create `initialize_live_auditor_agent` in `artisans.py`.
    *   This agent should use an LLM (Gemini, keeping consistency) with the `GENERAL_INSPECTOR_CHARTER_PROMPT` (or a more specialized one) to perform a semantic audit of the code generated by the live Coder Agent.
    *   The agent should analyze the code for correctness against the plan, adherence to best practices, potential bugs, etc.
    *   Output should be an `AuditOutput` object, potentially with a path to a more detailed report.
    *   Update `workshop_manager.py` to call this new live auditor.

3.  **Structured Output from Planner:**
    *   The `PLANNER_CHARTER_PROMPT` hints at YAML or JSON output. The current live planner treats the LLM output as a list of tasks (splitting by newline).
    *   Refine the planner agent to more reliably request and parse structured output (e.g., JSON) from the LLM. This might involve:
        *   Modifying the `PLANNER_CHARTER_PROMPT` to be more explicit about the desired JSON schema.
        *   Using features of the `google-generativeai` library that might support JSON mode or response schema validation, if available.
        *   Implementing robust parsing and error handling for the structured output.
    *   Update the `PlanOutput` data model if necessary to accommodate richer structured data.

4.  **Enhanced Error Handling and Retry Mechanisms:**
    *   Improve error handling for LLM API calls (e.g., specific exceptions for Gemini, rate limits, network issues).
    *   Implement retry mechanisms (e.g., using a library like `tenacity`) for transient API errors.

5.  **Expand Testing:**
    *   Develop more comprehensive automated tests for the agent pipeline.
    *   Include tests for various prompt types, edge cases, and potential failure modes of the LLM interactions.
    *   Test the quality of generated plans and code more systematically.

6.  **Explore CrewAI/LangGraph for Multi-Agent Orchestration (Longer Term):**
    *   While direct LLM calls were used for simplicity in this iteration, refer to `docs/technology_stack.md`.
    *   Begin exploring how `CrewAI` or `LangGraph` could be used to orchestrate the Planner, Coder, and Auditor agents as a more formal "crew," potentially enabling more complex interactions, feedback loops, and specialized sub-agents. This aligns with the original design intent.

## General Reminders for Next Jules:

*   **Adherence to `CONTRIBUTING.md`**: Ensure all contributions follow the guidelines outlined in `CONTRIBUTING.md`.
*   **Roadmap Alignment**: Refer to `docs/roadmap/V1.md` (and subsequent versions as they are created) for task identification and prioritization. The current focus is on fleshing out the V1 pipeline.
*   **Project Vision**: Keep the overall project vision from `docs/VERSION_ROADMAP.md` and other strategic documents in mind.
*   **`AGENTS.md`**: Always check for and follow instructions in any `AGENTS.md` files. This file is your primary source for task handovers.

Good luck with the next phase of development!
